# Object Segmentation and Movement Tool

This project is a Python-based tool for object segmentation and movement using state-of-the-art deep learning models. The script leverages Grounding DINO, SAM (Segment Anything Model), and ControlNet models for segmentation and object inpainting.

## Features

- **Object Segmentation**: Detect and segment objects in an image based on a text description.
- **Object Movement**: Relocate objects in the image using inpainting techniques guided by ControlNet.
- **High-quality Output**: Generates photorealistic results with seamless integration into the original image.

## Requirements

### Libraries
- Python 3.11+
- PyTorch
- NumPy
- OpenCV
- Pillow
- Diffusers
- Transformers
- Matplotlib

Install the required libraries using:

```bash
pip install -r requirements.txt
```

### Models
The script uses pretrained models:
- **Grounding DINO**: `IDEA-Research/grounding-dino-base`
- **SAM**: `facebook/sam-vit-huge`
- **ControlNet Inpainting**: `lllyasviel/control_v11p_sd15_inpaint`
- **Stable Diffusion**: `runwayml/stable-diffusion-inpainting`

We use the implementation of these models present on Diffusers and Transformers and are automatically downloaded when the script is executed

## Usage

### Command-line Arguments

```bash
python move_object.py --image <image_path> --class <object_class> [--output <output_path>] [--x <x_shift>] [--y <y_shift>]
```

#### Arguments:
- `--image`: Path to the input image.
- `--class`: Text description of the object to segment.
- `--output`: Path to save the output image (optional).
- `--x`: Horizontal shift in pixels for moving the object (default: 0).
- `--y`: Vertical shift in pixels for moving the object (default: 0).

### Examples

#### Segmentation Only
To segment an object:

```bash
python move_object.py --image input.jpg --class "chair" --output segmented_output.png
```

#### Object Movement
To move an object:

```bash
python move_object.py --image input.jpg --class "chair" --output moved_output.png --x 50 --y 100
```

## Output

1. **Segmentation Visualization**:
   - A semi-transparent red overlay is applied to the segmented object.
   - Saved in `./sam_mask/` or the specified output path.

2. **Object Movement**:
   - Relocated object with photorealistic inpainting.
   - Saved in `./outputs/` or the specified output path.

## Directory Structure

The script creates and uses the following directories for intermediate results:
- `./dino_bbox/`: Bounding box images from Grounding DINO.
- `./sam_mask/`: Masks generated by SAM.
- `./extracted_object/`: Cropped object images.
- `./inpainted_background/`: Background inpainting results (first inpainting without shifting of the object)
- `./outputs/`: Final results after object movement.

## Notes

- Ensure CUDA is available for GPU acceleration; otherwise, computations will default to the CPU.
- The script handles cases where the described object is not detected by using pre-defined bounding boxes for specific objects.

## License

This project is licensed under the MIT License. See the LICENSE file for details.

## Acknowledgments

- [Hugging Face](https://huggingface.co/) for the Transformers library.
- [Diffusers](https://github.com/huggingface/diffusers) for inpainting and ControlNet pipelines.
- Open source contributors for Grounding DINO and SAM.

